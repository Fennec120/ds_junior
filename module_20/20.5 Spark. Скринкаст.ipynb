{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qa5cbSmcgY4u"
   },
   "source": [
    "Запускаем ячейку, чтобы установить библиотеки и нужное для работы со Spark окружение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pMumVgN-FDbK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ЋиЁЎЄ  ў бЁ­в ЄбЁбҐ Є®¬ ­¤л.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark --quiet\n",
    "!pip install -U -q PyDrive --quiet \n",
    "!apt install openjdk-8-jdk-headless &> /dev/null\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xq94pbQ-gv9l"
   },
   "source": [
    "Инициализируем спарк-сессию. SparkSession — это, по сути, точка входа для работы с Apache Spark в программе-клиенте. После инициализации нам будут доступны базовые возможности фреймворка Spark для создания распределённых массивов данных RDD, DataFrame и DataSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UhT8nFFBFwD8"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\\\n\u001b[0;32m      2\u001b[0m         \u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPySpark_Tutorial\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m----> 4\u001b[0m         \u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\java_gateway.py:104\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName('PySpark_Tutorial')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGIgqEGYBulb"
   },
   "source": [
    "[Сайт, где можно найти всё или почти всё о синтаксисе PySpark](https://sparkbyexamples.com/pyspark/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxfamL1SF_7h"
   },
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKN9B8tw0H4s"
   },
   "source": [
    "[Сайт, откуда можно скачать датасет с квартирами](https://www.kaggle.com/datasets/mrdaniilak/russia-real-estate-2021?select=input_data.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5UP-zR3hffv"
   },
   "source": [
    "При запуске ячейки, расположенной ниже, всплывающее меню прокрутите до конца и нажмите «Разрешить»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4v0RH3Noabt",
    "outputId": "7efbb9f1-8887-408c-891f-4aaa347ba51b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive #Импортируем библиотеку для работы с Google Drive\n",
    "drive.mount(\"/content/drive\") #Монтируем Google-диск. После этой операции мы можем считывать файлы со своего Google-диска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NyLoWJzov1u"
   },
   "source": [
    "Чтобы следующая ячейка запустилась без ошибок, нужно сохранить датасет на свой Google-диск."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCQKNAKJLm6E"
   },
   "source": [
    "Загружаем датасет с недвижимостью в России с диска.\n",
    "\n",
    "Для загрузки данных используем read.csv():\n",
    "\n",
    "*  .option(\"delimiter\",\";\") — указываем разделитель для считываемого CSV-файла.\n",
    "*  .option(\"header\",True) — читаем первую строку CSV как заголовки столбцов будущей таблицы.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLK0V9Jp5O0L"
   },
   "outputs": [],
   "source": [
    "realty = spark.read.option(\"header\",True).option(\"delimiter\",\";\").csv(\"/content/drive/MyDrive/Colab Notebooks/input_data.csv\") \n",
    "#Читаем наши данные. \"/content/drive/MyDrive/Colab Notebooks/input_data.csv\" — это путь к файлу с данными на диске. У вас путь может быть другой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ts6VgRaD1Z85",
    "outputId": "dcf2b82f-56b4-4911-e8bf-035baea98380"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(realty) #Смотрим тип нашей переменной"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPgGNzCzMPRx"
   },
   "source": [
    "Чтобы отобразить первые строки датафрейма, используем метод show(), который по умолчанию в PySpark показывает 20 строк.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mc0VSN7cXT4t",
    "outputId": "d1185032-41e9-4c2b-8726-bafaf4a2695f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+------+-----+-----+------------+----------+-----------+-------------+-----------+-----------+---------+---------+--------+\n",
      "|      date|   price|level|levels|rooms| area|kitchen_area|   geo_lat|    geo_lon|building_type|object_type|postal_code|street_id|id_region|house_id|\n",
      "+----------+--------+-----+------+-----+-----+------------+----------+-----------+-------------+-----------+-----------+---------+---------+--------+\n",
      "|2021-01-01| 2451300|   15|    31|    1| 30.3|           0|56.7801124| 60.6993548|            0|          2|     620000|     null|       66| 1632918|\n",
      "|2021-01-01| 1450000|    5|     5|    1|   33|           6|44.6081542| 40.1383814|            0|          0|     385000|     null|        1|    null|\n",
      "|2021-01-01|10700000|    4|    13|    3|   85|          12|55.5400601| 37.7251124|            3|          0|     142701|   242543|       50|  681306|\n",
      "|2021-01-01| 3100000|    3|     5|    3|   82|           9|44.6081542| 40.1383814|            0|          0|     385000|     null|        1|    null|\n",
      "|2021-01-01| 2500000|    2|     3|    1|   30|           9|44.7386846| 37.7136681|            3|          2|     353960|   439378|       23| 1730985|\n",
      "|2021-01-01| 1450000|    5|     5|    2|   47|           6|48.5111715| 44.5668459|            2|          0|     400096|   260588|       34| 1009994|\n",
      "|2021-01-01| 9000000|    2|     4|    3|107.4|        21.3|55.0099136| 82.9348589|            4|          0|     630102|   233285|       54| 2823596|\n",
      "|2021-01-01| 2990000|    1|     2|    3|   54|           7|51.8347031|107.6005709|            0|          0|     670034|     null|        3|    null|\n",
      "|2021-01-01| 2300000|   16|    18|    1| 39.7|        11.5|45.0038685| 39.0865107|            4|          0|     350065|   523822|       23| 1284243|\n",
      "|2021-01-01| 2290000|    2|     2|    2| 53.2|          16| 53.164362|  45.033956|            5|          0|     440003|     null|       58|    null|\n",
      "|2021-01-01| 3350000|    1|    19|   -1| 10.1|           2| 55.778026|  37.540147|            3|          0|     123007|   580339|       77| 2069062|\n",
      "|2021-01-01| 5000000|    8|     9|    3|  100|           0| 57.104626|   65.58797|            4|          0|     625046|   450441|       72| 2825516|\n",
      "|2021-01-01| 3214804|    5|    17|   -1|32.44|           0|55.4624497|  37.700041|            0|          2|     102321|     null|       50|  799789|\n",
      "|2021-01-01| 3750000|   19|    20|    2|   40|        -100|55.0552885| 82.8941912|            3|          0|     630082|   557992|       54| 2715855|\n",
      "|2021-01-01|11100000|    3|    17|    2| 62.4|         9.7|55.6905542| 37.8635456|            0|          0|     140002|     null|       50|    null|\n",
      "|2021-01-01| 8865000|   10|    13|    2| 66.7|        14.6|59.8511787| 30.4116573|            4|          2|     192288|   569976|       78| 1690045|\n",
      "|2021-01-01| 2650000|    6|     7|   -1|   26|           4|57.1529744| 65.5344099|            3|          2|     625062|   229121|       72|  880012|\n",
      "|2021-01-01| 2300000|    2|     4|    3| 62.4|         7.7|48.7811242| 44.5777823|            4|          0|     400009|     null|       34|    null|\n",
      "|2021-01-01| 2500000|    5|     9|    3|   61|         7.5|57.6881859|   39.76105|            2|          0|     150063|   190251|       76| 1940462|\n",
      "|2021-01-01| 4650000|    5|     9|    2| 56.8|           8|54.7422365| 55.9962363|            4|          0|     450097|   458583|        2|  918972|\n",
      "+----------+--------+-----+------+-----+-----+------------+----------+-----------+-------------+-----------+-----------+---------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "realty.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O3Y719uFMroZ",
    "outputId": "608a5eba-c321-44e2-dde7-98bced5271c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+------+-----+----+------------+--------+--------+-------------+-----------+-----------+---------+---------+--------+\n",
      "|    date|   price|level|levels|rooms|area|kitchen_area| geo_lat| geo_lon|building_type|object_type|postal_code|street_id|id_region|house_id|\n",
      "+--------+--------+-----+------+-----+----+------------+--------+--------+-------------+-----------+-----------+---------+---------+--------+\n",
      "|2021-...| 2451300|   15|    31|    1|30.3|           0|56.78...|60.69...|            0|          2|     620000|     null|       66| 1632918|\n",
      "|2021-...| 1450000|    5|     5|    1|  33|           6|44.60...|40.13...|            0|          0|     385000|     null|        1|    null|\n",
      "|2021-...|10700000|    4|    13|    3|  85|          12|55.54...|37.72...|            3|          0|     142701|   242543|       50|  681306|\n",
      "|2021-...| 3100000|    3|     5|    3|  82|           9|44.60...|40.13...|            0|          0|     385000|     null|        1|    null|\n",
      "|2021-...| 2500000|    2|     3|    1|  30|           9|44.73...|37.71...|            3|          2|     353960|   439378|       23| 1730985|\n",
      "+--------+--------+-----+------+-----+----+------------+--------+--------+-------------+-----------+-----------+---------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "realty.show(5,8) #Показать первые 5 строк, по 8 символов на каждый столбец"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9nqTeBfQYlV"
   },
   "source": [
    "Метод printSchema() показывает структуру таблицы (датафрейма): имена и тип столбцов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KsfhVkWOFXd",
    "outputId": "1c188382-dc9b-4035-95a1-ecf68b591b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- levels: string (nullable = true)\n",
      " |-- rooms: string (nullable = true)\n",
      " |-- area: string (nullable = true)\n",
      " |-- kitchen_area: string (nullable = true)\n",
      " |-- geo_lat: string (nullable = true)\n",
      " |-- geo_lon: string (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- object_type: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      " |-- street_id: string (nullable = true)\n",
      " |-- id_region: string (nullable = true)\n",
      " |-- house_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "realty.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZF1JlFb1OYC"
   },
   "source": [
    "Как видите, все столбцы нашего датафрейма — строкового типа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2LwfP_3SNlX"
   },
   "source": [
    "### Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HorHpKiS5Lx"
   },
   "source": [
    "Поменяем тип столбцов датафрейма.\n",
    "\n",
    "Это можно сделать разными способами, например:\n",
    "*   с помощью метода select: your_dataframe.select(col(\"column_name\").cast(\"new_type\") ...)\n",
    "*   через selectExpr: your_dataframe.selectExpr(\"CAST(culumn_name AS new_type) as column_name\", ...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJvIO7Z4OQQU"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,sum,avg,max,count,expr\n",
    "\n",
    "realty_formatted = realty.select(\n",
    "    col(\"level\").cast(\"Int\"),\n",
    "    col(\"levels\").cast(\"Int\"),\n",
    "    col(\"rooms\").cast(\"Int\"),\n",
    "    col(\"area\").cast(\"Double\"),\n",
    "    col(\"kitchen_area\").cast(\"Double\"),\n",
    "    col(\"geo_lat\").cast(\"Double\"),\n",
    "    col(\"geo_lon\").cast(\"Double\"),\n",
    "    col(\"building_type\").cast(\"Int\"),\n",
    "    col(\"object_type\").cast(\"Int\"),\n",
    "    col(\"id_region\").cast(\"Int\"),\n",
    "    col(\"price\").cast(\"Double\")\n",
    ")\n",
    "#Мы выбрали из датафрейма колонки level, levels, rooms, area, kitchen_area, geo_lat, geo_lon, building_type, object_type, object_type, id_region, price,\n",
    "#одновременно изменив их тип при помощи оператора cast(). Те колонки, которые мы не перечислили внутри Select, не попали в новый датафрейм realty_formatted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vs2UbcqDl6o4"
   },
   "source": [
    "Рассмотрим структуру получившегося датафрейма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-jZWFK6Qo43",
    "outputId": "4ee06c6b-9d66-4379-d102-20f022f2b367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- level: integer (nullable = true)\n",
      " |-- levels: integer (nullable = true)\n",
      " |-- rooms: integer (nullable = true)\n",
      " |-- area: double (nullable = true)\n",
      " |-- kitchen_area: double (nullable = true)\n",
      " |-- geo_lat: double (nullable = true)\n",
      " |-- geo_lon: double (nullable = true)\n",
      " |-- building_type: integer (nullable = true)\n",
      " |-- object_type: integer (nullable = true)\n",
      " |-- id_region: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "realty_formatted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYi1zKBEVdsC"
   },
   "source": [
    "Посчитаем количество строк. Это можно сделать при помощи метода .count()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_JGQFkFcH-r",
    "outputId": "d0c0b265-bf1c-4f4f-963f-c8c32a5ee66b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11358150"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realty_formatted.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQdT2ohBVr1D"
   },
   "source": [
    "Выберем квартиры, продаваемые в Московском регионе.\n",
    "\n",
    "Фильтровать данные в датафрейме позволяет метод .filter() (аналог WHERE в SQL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7Axz3P-LAeB"
   },
   "outputs": [],
   "source": [
    "realty_S0 = realty_formatted.filter(realty_formatted.id_region == 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-XiMhewWgd5"
   },
   "source": [
    "Как видим, размер датафрейма значительно уменьшился.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofeYuYVdLbFW",
    "outputId": "1379ff53-8678-4aa9-b861-6e02d7efc6ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754267"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realty_S0.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xe8S070UmJcv"
   },
   "source": [
    "Отобразим первые строки датафрейма. Видим, что в нём есть квартиры с отрицательной площадью кухни."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cVWtzuHRHpHc",
    "outputId": "e44ad45a-d789-4c3f-edce-25d298f0b32c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+-----+------------+----------+----------+-------------+-----------+---------+---------+\n",
      "|level|levels|rooms| area|kitchen_area|   geo_lat|   geo_lon|building_type|object_type|id_region|    price|\n",
      "+-----+------+-----+-----+------------+----------+----------+-------------+-----------+---------+---------+\n",
      "|    4|    13|    3| 85.0|        12.0|55.5400601|37.7251124|            3|          0|       50|   1.07E7|\n",
      "|    5|    17|   -1|32.44|         0.0|55.4624497| 37.700041|            0|          2|       50|3214804.0|\n",
      "|    3|    17|    2| 62.4|         9.7|55.6905542|37.8635456|            0|          0|       50|   1.11E7|\n",
      "|    2|    17|    1|36.99|       11.72|55.5639247|37.8255637|            4|          2|       50|4296056.0|\n",
      "|    5|     5|    3| 66.1|         9.0|55.9072587|38.0579851|            2|          0|       50|3899000.0|\n",
      "|    4|     9|    3| 52.4|         6.0|55.7496117|38.0076555|            2|          0|       50|6000000.0|\n",
      "|    7|    17|    1| 39.0|        10.0|55.8075249|   37.9434|            2|          0|       50|5175000.0|\n",
      "|    7|     9|    1| 32.0|         6.0|55.9819313|37.7430357|            3|          0|       50|4090000.0|\n",
      "|    3|    17|    1|41.27|         0.0|55.4624497| 37.700041|            0|          2|       50|3718427.0|\n",
      "|    4|     5|    2| 31.9|      -100.0|55.8025732|38.9795924|            4|          0|       50|1750000.0|\n",
      "|   12|    17|   -1|28.07|         0.0|55.4624497| 37.700041|            0|          2|       50|2927701.0|\n",
      "|    1|     4|   -1| 36.0|         6.0| 55.920769| 37.880171|            3|          0|       50|4450000.0|\n",
      "|    9|    17|    1|41.34|         0.0|55.4624497| 37.700041|            0|          2|       50|3749538.0|\n",
      "|    3|     5|    2| 41.0|         5.4|55.6971281|37.8639911|            4|          0|       50|6700000.0|\n",
      "|    7|     8|    2| 70.8|      -100.0|55.5294003|37.7123165|            3|          2|       50|  1.005E7|\n",
      "|   12|    17|    1|38.09|        12.9|55.5639247|37.8255637|            0|          2|       50|4525511.0|\n",
      "|   16|    17|    1|38.09|        12.9|55.5639247|37.8255637|            0|          2|       50|4554193.0|\n",
      "|   16|    17|   -1| 28.3|         5.0|55.9105782|37.7363579|            3|          2|       50|4800000.0|\n",
      "|    1|     8|    2| 75.9|        23.7| 55.551725|37.7061984|            0|          0|       50|7950000.0|\n",
      "|    3|    17|    1|32.69|        9.57|55.5639247|37.8255637|            4|          2|       50|5011344.0|\n",
      "+-----+------+-----+-----+------------+----------+----------+-------------+-----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "realty_S0.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ysjwmfpddm7H"
   },
   "source": [
    "Посмотрим, есть ли в датафрейме другие квартиры, площадь кухни которых отрицательная.\n",
    "\n",
    "Сгруппируем квартиры с отрицательной площадью и выясним, в скольких из них параметры кухни имеют «неправильное» значение.\n",
    "\n",
    "Код ниже эквивалентен следующему SQL-запросу: SELECT kitchen_area, count(price) as count_items \n",
    "FROM realty_S0 WHERE kitchen_area < 0\n",
    "GROUP BY kitchen_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EpMSojhHXP0p",
    "outputId": "a9358996-1527-46d8-ba9d-302ac2c26128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|kitchen_area|count_items|\n",
      "+------------+-----------+\n",
      "|      -100.0|      56262|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "realty_S0.filter(col(\"kitchen_area\") < 0).groupBy(\"kitchen_area\").agg(count(\"price\").alias(\"count_items\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFIAaO63ojcH"
   },
   "source": [
    "Выберем только те квартиры, площадь кухни которых ≥ 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LKRHmiVL6Kj"
   },
   "outputs": [],
   "source": [
    "realty_S1 = realty_S0.filter(col(\"kitchen_area\") >= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngLz6Dfco0EG"
   },
   "source": [
    "Снова посчитаем количество строк в датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_giO8IocMcn8",
    "outputId": "38e42600-aec9-4e72-a968-3dfe65e23c96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "698005"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realty_S1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3H_8NM4o8Vd"
   },
   "source": [
    "Перейдём к feature-инженирингу.\n",
    "\n",
    "Очевидно, что координаты дома, где находится квартира, не слишком коррелируют с ценой. На цену квартиры, как правило, влияет расстояние до центра города.\n",
    "\n",
    "Возьмём координаты центра Москвы и коэффициенты перевода градусов широты и долготы (на широтах в 55 градусов в Северном полушарии) в километры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsX5iNvBZSeh"
   },
   "outputs": [],
   "source": [
    "Moscow_lat = 55.753995\n",
    "Moscow_lon = 37.614069\n",
    "\n",
    "lat_to_km = 113.3 #Количество километров в одном градусе долготы\n",
    "lon_to_km = 64.0 #Количество километров в одном градусе широты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btJknXrdp5ro"
   },
   "source": [
    "Напишем функцию  coord_to_distance, которая будет брать на вход координаты дома, а выдавать расстояние от дома до центра Москвы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o096zBABYTrs"
   },
   "outputs": [],
   "source": [
    "def coord_to_distance(latitude,longitude):\n",
    "    distance = (((latitude - Moscow_lat)*lat_to_km)**2 + ((longitude - Moscow_lon)*lon_to_km)**2) ** 0.5\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRcWwspIqX5e"
   },
   "source": [
    "Применим нашу функцию к столбцам geo_lat и geo_lon. Создадим новый — distance_to_center, а названные столбцы удалим.\n",
    "\n",
    "Новый столбец в датафрейме можно создать с помощью метода withColumn(). Удалять стоблцы можно при помощи drop()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XN2jlzAOFiWC"
   },
   "outputs": [],
   "source": [
    "realty_S2 = realty_S1.withColumn(\"distance_to_center\",coord_to_distance(col(\"geo_lat\"),col(\"geo_lon\"))).drop(\"geo_lat\",\"geo_lon\",\"id_region\")\n",
    "#Подробное описание: наш_датафрейм.withColumn(\"имя_новой_колонки\",Операции над старыми колонками для получения новой).drop(\"столбец_который_мы_хотим_удалить1\",\"столбец_который_мы_хотим_удалить2\",...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUwNr5fPt6Xl"
   },
   "source": [
    "Создадим два бинарных признака:\n",
    "\n",
    "\n",
    "*   столбец not_first_level, принимающий 1, если квартира не на первом, и 0, если квартира на первом этаже;\n",
    "*   столбец not_last_level, принимающий 1, если квартира не на последнем, и 0, если квартира на последнем этаже.\n",
    "\n",
    "Чтобы получить значения новых столбцов, используем оператор when:\n",
    "\n",
    "when(условие, значение, если true).otherwise(значение, если false).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rmi13M1dF9cc"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when  \n",
    "\n",
    "realty_S3 = realty_S2.withColumn(\"not_first_level\",when(col(\"level\") == 1,0).otherwise(1)).withColumn(\"not_last_level\",when(col(\"level\") == col(\"levels\"),0).otherwise(1))\n",
    "#Описание работы when: when(условие, значение, если true).otherwise(значение, если false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IRvXMzB5t_ek",
    "outputId": "22cb978d-b0f0-4186-94f7-9cf28e5f04f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+-----+------------+-------------+-----------+---------+------------------+---------------+--------------+\n",
      "|level|levels|rooms| area|kitchen_area|building_type|object_type|    price|distance_to_center|not_first_level|not_last_level|\n",
      "+-----+------+-----+-----+------------+-------------+-----------+---------+------------------+---------------+--------------+\n",
      "|    4|    13|    3| 85.0|        12.0|            3|          0|   1.07E7|25.259194068696385|              1|             1|\n",
      "|    5|    17|   -1|32.44|         0.0|            0|          2|3214804.0| 33.48720302595378|              1|             1|\n",
      "|    3|    17|    2| 62.4|         9.7|            0|          0|   1.11E7| 17.50983382864081|              1|             1|\n",
      "|    2|    17|    1|36.99|       11.72|            4|          2|4296056.0|25.435581975122673|              1|             1|\n",
      "|    5|     5|    3| 66.1|         9.0|            2|          0|3899000.0|  33.2971381244445|              1|             0|\n",
      "+-----+------+-----+-----+------------+-------------+-----------+---------+------------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "realty_S3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtXyzzOCpKJ1"
   },
   "source": [
    "Среди столбцов датафрейма можно увидеть колонку building_type — тип строения. Значения, которые она принимает, показаны ниже в ячейке. Эта колонка — категориальный признак. А как мы знаем, прежде чем начать обучать модель, нужно трансформировать категориальные признаки в векторы или в колонки бинарных признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTUCy1JAzh5N"
   },
   "source": [
    "Значения building_type: \n",
    "\n",
    "0 — unknown (неизвестный).\n",
    "\n",
    "1 — other (другой).\n",
    "\n",
    "2 — panel (панельный).\n",
    "\n",
    "3 — monolithic (монолитный).\n",
    "\n",
    "4 — brick (кирпичный).\n",
    "\n",
    "5 — blocky (блочный).\n",
    "\n",
    "6 — wooden (деревянный).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fOwAjC_3kfb"
   },
   "source": [
    "Векторизуем категориальный признак с помощью OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6VNM_3Gt_hm"
   },
   "outputs": [],
   "source": [
    "from pyspark import mllib\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ESV4j7Kvsyu"
   },
   "outputs": [],
   "source": [
    "features_inp = [\"building_type\"] #Имя категориальной колонки\n",
    "\n",
    "features_out = [\"building_type_vec\"] #Имя новой колонки, которая получится после преобразований\n",
    "\n",
    "encoder = OneHotEncoder(inputCols = features_inp, outputCols = features_out) #Cоздадим объект encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuT_xzX1rRLZ"
   },
   "source": [
    "Сначала при помощи метода fit() обучим encoder, в который «скормим» наш датафрейм.\n",
    "\n",
    "Затем, воспользовавшись обученным encoder, преобразуем датафрейм методом transform() и удалим старую непреобразованную колонку при помощи drop()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6CUMwxlYPh6"
   },
   "outputs": [],
   "source": [
    "fitted_encoder = encoder.fit(realty_S3) \n",
    "realty_S4 = fitted_encoder.transform(realty_S3).drop(\"building_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTHh7Ab7rtoL"
   },
   "source": [
    "Видим, что колонка преобразовалась в вектор (building_type_vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Ejvr0dhGllA",
    "outputId": "4422f69d-0e28-4b45-8c5c-f7487f9c85ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+-----+------------+-----------+---------+------------------+---------------+--------------+-----------------+\n",
      "|level|levels|rooms| area|kitchen_area|object_type|    price|distance_to_center|not_first_level|not_last_level|building_type_vec|\n",
      "+-----+------+-----+-----+------------+-----------+---------+------------------+---------------+--------------+-----------------+\n",
      "|    4|    13|    3| 85.0|        12.0|          0|   1.07E7|25.259194068696385|              1|             1|    (6,[3],[1.0])|\n",
      "|    5|    17|   -1|32.44|         0.0|          2|3214804.0| 33.48720302595378|              1|             1|    (6,[0],[1.0])|\n",
      "|    3|    17|    2| 62.4|         9.7|          0|   1.11E7| 17.50983382864081|              1|             1|    (6,[0],[1.0])|\n",
      "|    2|    17|    1|36.99|       11.72|          2|4296056.0|25.435581975122673|              1|             1|    (6,[4],[1.0])|\n",
      "|    5|     5|    3| 66.1|         9.0|          0|3899000.0|  33.2971381244445|              1|             0|    (6,[2],[1.0])|\n",
      "+-----+------+-----+-----+------------+-----------+---------+------------------+---------------+--------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "realty_S4.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuoxUzeKr5Q8"
   },
   "source": [
    "###Немного о векторах в библиотеке PySpark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbBYgiwvvs19"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vector, Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9qdETWnApym"
   },
   "source": [
    "DenseVector (плотный вектор) — обычный вектор, задавая который нужно прописать значение каждой его компоненты.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALdvoEZNt_nX",
    "outputId": "5aa90794-9701-40c6-df0d-d6556220cb32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 2.0, 3.0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1 = Vectors.dense([1, 2, 3]) #Создаём плотный вектор\n",
    "vec1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PIVooYKAteO"
   },
   "source": [
    "SparseVector (разреженный вектор) — вектор, который состоит преимущественно из нулей. Только несколько его координат имеют отличные от нуля значения.\n",
    "\n",
    "Задавая такой вектор, нужно указать кортеж, в котором перечисляется размерность, список индексов, где есть ненулевые значения, и сами ненулевые значения. \n",
    "\n",
    "Запись, показанная в ячейке, создаст вектор, состоящий из шести компонент. На позиции 1 и 4 будут находиться числа 2 и –3,5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOSGXnfHt_tP",
    "outputId": "74490897-f360-48ec-f71b-bf7e9bd08bcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(6, {1: 2.0, 4: -3.5})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2 = Vectors.sparse(6, [1, 4], [2.0, -3.5]) #Создаём разреженный вектор\n",
    "vec2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd7CLtWEDD97"
   },
   "source": [
    "Чтобы преобразовать разреженный вектор в плотный, нужно сначала преобразовать его в NumPy array методом toArray(), а затем обернуть в denseVector, как это показано в ячейке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-G4W0d-mt_wA",
    "outputId": "8431ecb4-412e-4451-a5ea-424c5a060e3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0, 2.0, 0.0, 0.0, -3.5, 0.0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectors.dense(vec2.toArray()) #Преобразуем разреженный вектор в плотный"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-s_5aRMMSfmz"
   },
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKOa-IxFReRt"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flu4DDVWSkAE"
   },
   "source": [
    "Выделим из датафрейма список фичей и столбец target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z51RMnOpt_16"
   },
   "outputs": [],
   "source": [
    "features = list(realty_S4.drop(\"price\").columns) #Список колонок фичей\n",
    "\n",
    "target = \"price\" #Колонка «таргет»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GXpmU0etA64"
   },
   "source": [
    "В отличие от знакомой вам библиотеки Sklearn, в PySpark ML перед обучением моделей необходимо объединять все колонки признаков в многомерные векторы. Провести векторизацию нам поможет объект класса VectorAssembler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxk8jiMHS6ZY"
   },
   "source": [
    "Векторизуем фичи с помощью VectorAssembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DHdkUGROXmT"
   },
   "outputs": [],
   "source": [
    "vectorizer = VectorAssembler(inputCols = features, outputCol = \"features_vec\") \n",
    "#Создаём переменную vectorizer — объект класса VectorAssembler, цель которого — превратить колонки с фичами в векторы\n",
    "\n",
    "realty_vectorised = vectorizer.transform(realty_S4)\n",
    "#Трансформируем при помощи vectorizer наш датафрейм и сохраним его в переменную realty_vectorised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyEzZiVauItx"
   },
   "source": [
    "Как видим, все колонки фичей превратились в одну — features_vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q35RT99XTOpo",
    "outputId": "2bd87fb2-c73e-447f-890c-48f1ae7eaa59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------+---------+\n",
      "|                                                                     features_vec|    price|\n",
      "+---------------------------------------------------------------------------------+---------+\n",
      "|  [4.0,13.0,3.0,85.0,12.0,0.0,25.259194068696385,1.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0]|   1.07E7|\n",
      "|  [5.0,17.0,-1.0,32.44,0.0,2.0,33.48720302595378,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0]|3214804.0|\n",
      "|    [3.0,17.0,2.0,62.4,9.7,0.0,17.50983382864081,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0]|   1.11E7|\n",
      "|[2.0,17.0,1.0,36.99,11.72,2.0,25.435581975122673,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0]|4296056.0|\n",
      "|          (15,[0,1,2,3,4,6,7,11],[5.0,5.0,3.0,66.1,9.0,33.2971381244445,1.0,1.0])|3899000.0|\n",
      "+---------------------------------------------------------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "realty_vectorised.select(\"features_vec\",\"price\").show(5,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMz6BjSrUWFX"
   },
   "source": [
    "Создадим модель, используя класс LinearRegression, куда в качестве параметров передадим имя колонки с вектором фичей (featuresCol = 'features_vec') и имя колонки, значение в которой хотим предсказывать (назовём её target), — labelCol = target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qd3LdkRBRNkw"
   },
   "outputs": [],
   "source": [
    "model = LinearRegression(featuresCol = 'features_vec', labelCol = target)\n",
    "#Создаём объект model — экземпляр класса LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bW27tdzMUc8Q"
   },
   "source": [
    "Разбиваем датасет на обучающую и тестовую выборку, используя randomSplit:\n",
    "\n",
    "80% строк — на обучающую выборку.\n",
    "\n",
    "20% строк — на тестовую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eccPKoBct_y6"
   },
   "outputs": [],
   "source": [
    "realty_train, realty_test = realty_vectorised.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bt6JbOYiUrtX"
   },
   "source": [
    "Для обучения модели воспользуемся поиском по сетке гиперпараметров с кросс-валидацией. Создадим два объекта: саму сетку гиперпараметров и evaluator — объект, чьей задачей будет оценивать качество предсказания нашей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rnrsVMAVpsb"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAF4n5xvVCcu"
   },
   "source": [
    "Определяем сетку гиперпараметров при помощи ParamGridBuilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7q13Lptt_4q"
   },
   "outputs": [],
   "source": [
    "grid = ParamGridBuilder().addGrid(model.regParam, [0.5, 5]).addGrid(model.elasticNetParam, [0.01, 0.1]).build()\n",
    "#Создана сетка гиперпараметров, в которой параметр regParam принимает значения 0.5, 5, а параметр elasticNetParam — значения 0.01, 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0A01B6wlhOj"
   },
   "source": [
    "Создаём объект eval — экземпляр класса RegressionEvaluator, куда в качестве параметров пишем имя столбцов с предсказанным и реальным значением.\n",
    "\n",
    "По умолчанию для оценки качества модели RegressionEvaluator использует метрику MSE — среднеквадратичную ошибку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUz5F3ekt_7h"
   },
   "outputs": [],
   "source": [
    "eval = RegressionEvaluator(predictionCol='prediction', labelCol = target)\n",
    "#Столбец с предсказаниями будет иметь имя 'prediction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lNPv7ERotiz"
   },
   "source": [
    "Теперь создаём CV-объект — экземпляр класса CrossValidator, куда в качестве параметров передаём ранее определённую модель model, сетку гиперпараметров grid и метрику качества eval.\n",
    "\n",
    "Для обучения модели, как и в Sklearn, будем использовать метод fit().\n",
    "\n",
    "Нужно подождать, пока модель обучается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1aFHct-Vt_-e"
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator = model, estimatorParamMaps = grid, evaluator = eval) #Создаём объект cv — экземпляр CrossValidator\n",
    "cv_model = cv.fit(realty_train) #Используем метод fit для обучения модели, скормив ему обучающую выборку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYhKBlO7y23m"
   },
   "source": [
    "Выберем лучшую модель, обученную при кросс-валидации, с помощью метода bestModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZX8QS-PC418y"
   },
   "outputs": [],
   "source": [
    "best_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpp920Gt0R2H"
   },
   "source": [
    "Посмотрим гиперпараметры лучшей модели при помощи extractParamMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1_boKz3z2Wg",
    "outputId": "456c304d-7113-4355-bb0b-068e7a9c7621"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LinearRegression_7100a461b518', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
       " Param(parent='LinearRegression_7100a461b518', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.1,\n",
       " Param(parent='LinearRegression_7100a461b518', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber'): 1.35,\n",
       " Param(parent='LinearRegression_7100a461b518', name='featuresCol', doc='features column name.'): 'features_vec',\n",
       " Param(parent='LinearRegression_7100a461b518', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
       " Param(parent='LinearRegression_7100a461b518', name='labelCol', doc='label column name.'): 'price',\n",
       " Param(parent='LinearRegression_7100a461b518', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber.'): 'squaredError',\n",
       " Param(parent='LinearRegression_7100a461b518', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
       " Param(parent='LinearRegression_7100a461b518', name='maxIter', doc='max number of iterations (>= 0).'): 100,\n",
       " Param(parent='LinearRegression_7100a461b518', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='LinearRegression_7100a461b518', name='regParam', doc='regularization parameter (>= 0).'): 0.5,\n",
       " Param(parent='LinearRegression_7100a461b518', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs.'): 'auto',\n",
       " Param(parent='LinearRegression_7100a461b518', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       " Param(parent='LinearRegression_7100a461b518', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPXmUgGiIagB"
   },
   "source": [
    "Чтобы было удобно применять обученную модель в разных кейсах, напишем функцию, которая принимает на вход параметры квартиры, а на выходе предсказывает её цену."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYv-r2HSTaek"
   },
   "outputs": [],
   "source": [
    "def price_predictor(level, levels, rooms, area, kitchen_area, geo_lat, geo_lon, building_type, object_type):\n",
    "  \n",
    "  #Зададим список колонок\n",
    "  columns = [\"level\", \"levels\", \"rooms\", \"area\", \"kitchen_area\", \"geo_lat\", \"geo_lon\", \"building_type\", \"object_type\"]\n",
    "  \n",
    "  #Представим входные параметры как список кортежей\n",
    "  #Один кортеж — одна строка в создаваемом датафрейме\n",
    "  values = [(level, levels, rooms, area, kitchen_area, geo_lat, geo_lon, building_type, object_type)]\n",
    "\n",
    "  #При помощи функции createDataframe создаём spark Dataframe, в качестве параметров передаём список имён столбцов и данные для таблицы\n",
    "  df = spark.createDataFrame(values,columns) \n",
    "\n",
    "  #Добавляем в датафрейм новые фичи, которые мы обсуждали на этапе data preparation. Как видим по коду, написанному ниже, все преобразования данных можно делать цепочкой\n",
    "  df_transformed = df.withColumn(\"distance_to_center\",coord_to_distance(col(\"geo_lat\"),col(\"geo_lon\"))).drop(\"geo_lat\",\"geo_lon\") \\\n",
    "                      .withColumn(\"not_first_level\",when(col(\"level\") == 1,0).otherwise(1)) \\\n",
    "                      .withColumn(\"not_last_level\",when(col(\"level\") == col(\"levels\"),0).otherwise(1)) \\\n",
    "\n",
    "  #Векторизуем категориальный признак — тип строения\n",
    "  df_transformed_vec = fitted_encoder.transform(df_transformed)\n",
    "\n",
    "  #Затем превратим в вектор все фичи нашего набора данных             \n",
    "  df_vectorised = vectorizer.transform(df_transformed_vec)\n",
    "\n",
    "  #Скормим полученный вектор фичей нашей обученной модели в метод transform() и на выходе получим цену квартиры\n",
    "  return best_model.transform(df_vectorised.select(\"features_vec\")).select(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrosv4ra8pu_",
    "outputId": "5019253e-1292-4d38-f696-c4b071d36dd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       prediction|\n",
      "+-----------------+\n",
      "|5860674.905795745|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Однушка в Королёве в кирпичном доме, вторичка\n",
    "price_predictor(level = 21, levels = 25, rooms = 1, area = 43.0, kitchen_area = 10.0, geo_lat = 55.917188, geo_lon = 37.804221, building_type = 4, object_type = 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4uS1DfXF-exq",
    "outputId": "6e2943b9-055e-41ef-c307-65e1926ad8fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         prediction|\n",
      "+-------------------+\n",
      "|1.840236967033116E7|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Трёшка в жилом небоскрёбе рядом с МЦК, вторичка\n",
    "price_predictor(level = 50, levels = 56, rooms = 3, area = 120.0, kitchen_area = 20.0, geo_lat = 55.835395, geo_lon = 37.658311, building_type = 4, object_type = 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tI8hVi2vAvG2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
